n = 100
dx = 0.010000
beta = 0.001000
Mat Object: 1 MPI processes
  type: seqaij
row 0: (0, 1.002)  (1, -0.001) 
row 1: (0, -0.001)  (1, 1.002)  (2, -0.001) 
row 2: (1, -0.001)  (2, 1.002)  (3, -0.001) 
row 3: (2, -0.001)  (3, 1.002)  (4, -0.001) 
row 4: (3, -0.001)  (4, 1.002)  (5, -0.001) 
row 5: (4, -0.001)  (5, 1.002)  (6, -0.001) 
row 6: (5, -0.001)  (6, 1.002)  (7, -0.001) 
row 7: (6, -0.001)  (7, 1.002)  (8, -0.001) 
row 8: (7, -0.001)  (8, 1.002)  (9, -0.001) 
row 9: (8, -0.001)  (9, 1.002)  (10, -0.001) 
row 10: (9, -0.001)  (10, 1.002)  (11, -0.001) 
row 11: (10, -0.001)  (11, 1.002)  (12, -0.001) 
row 12: (11, -0.001)  (12, 1.002)  (13, -0.001) 
row 13: (12, -0.001)  (13, 1.002)  (14, -0.001) 
row 14: (13, -0.001)  (14, 1.002)  (15, -0.001) 
row 15: (14, -0.001)  (15, 1.002)  (16, -0.001) 
row 16: (15, -0.001)  (16, 1.002)  (17, -0.001) 
row 17: (16, -0.001)  (17, 1.002)  (18, -0.001) 
row 18: (17, -0.001)  (18, 1.002)  (19, -0.001) 
row 19: (18, -0.001)  (19, 1.002)  (20, -0.001) 
row 20: (19, -0.001)  (20, 1.002)  (21, -0.001) 
row 21: (20, -0.001)  (21, 1.002)  (22, -0.001) 
row 22: (21, -0.001)  (22, 1.002)  (23, -0.001) 
row 23: (22, -0.001)  (23, 1.002)  (24, -0.001) 
row 24: (23, -0.001)  (24, 1.002)  (25, -0.001) 
row 25: (24, -0.001)  (25, 1.002)  (26, -0.001) 
row 26: (25, -0.001)  (26, 1.002)  (27, -0.001) 
row 27: (26, -0.001)  (27, 1.002)  (28, -0.001) 
row 28: (27, -0.001)  (28, 1.002)  (29, -0.001) 
row 29: (28, -0.001)  (29, 1.002)  (30, -0.001) 
row 30: (29, -0.001)  (30, 1.002)  (31, -0.001) 
row 31: (30, -0.001)  (31, 1.002)  (32, -0.001) 
row 32: (31, -0.001)  (32, 1.002)  (33, -0.001) 
row 33: (32, -0.001)  (33, 1.002)  (34, -0.001) 
row 34: (33, -0.001)  (34, 1.002)  (35, -0.001) 
row 35: (34, -0.001)  (35, 1.002)  (36, -0.001) 
row 36: (35, -0.001)  (36, 1.002)  (37, -0.001) 
row 37: (36, -0.001)  (37, 1.002)  (38, -0.001) 
row 38: (37, -0.001)  (38, 1.002)  (39, -0.001) 
row 39: (38, -0.001)  (39, 1.002)  (40, -0.001) 
row 40: (39, -0.001)  (40, 1.002)  (41, -0.001) 
row 41: (40, -0.001)  (41, 1.002)  (42, -0.001) 
row 42: (41, -0.001)  (42, 1.002)  (43, -0.001) 
row 43: (42, -0.001)  (43, 1.002)  (44, -0.001) 
row 44: (43, -0.001)  (44, 1.002)  (45, -0.001) 
row 45: (44, -0.001)  (45, 1.002)  (46, -0.001) 
row 46: (45, -0.001)  (46, 1.002)  (47, -0.001) 
row 47: (46, -0.001)  (47, 1.002)  (48, -0.001) 
row 48: (47, -0.001)  (48, 1.002)  (49, -0.001) 
row 49: (48, -0.001)  (49, 1.002)  (50, -0.001) 
row 50: (49, -0.001)  (50, 1.002)  (51, -0.001) 
row 51: (50, -0.001)  (51, 1.002)  (52, -0.001) 
row 52: (51, -0.001)  (52, 1.002)  (53, -0.001) 
row 53: (52, -0.001)  (53, 1.002)  (54, -0.001) 
row 54: (53, -0.001)  (54, 1.002)  (55, -0.001) 
row 55: (54, -0.001)  (55, 1.002)  (56, -0.001) 
row 56: (55, -0.001)  (56, 1.002)  (57, -0.001) 
row 57: (56, -0.001)  (57, 1.002)  (58, -0.001) 
row 58: (57, -0.001)  (58, 1.002)  (59, -0.001) 
row 59: (58, -0.001)  (59, 1.002)  (60, -0.001) 
row 60: (59, -0.001)  (60, 1.002)  (61, -0.001) 
row 61: (60, -0.001)  (61, 1.002)  (62, -0.001) 
row 62: (61, -0.001)  (62, 1.002)  (63, -0.001) 
row 63: (62, -0.001)  (63, 1.002)  (64, -0.001) 
row 64: (63, -0.001)  (64, 1.002)  (65, -0.001) 
row 65: (64, -0.001)  (65, 1.002)  (66, -0.001) 
row 66: (65, -0.001)  (66, 1.002)  (67, -0.001) 
row 67: (66, -0.001)  (67, 1.002)  (68, -0.001) 
row 68: (67, -0.001)  (68, 1.002)  (69, -0.001) 
row 69: (68, -0.001)  (69, 1.002)  (70, -0.001) 
row 70: (69, -0.001)  (70, 1.002)  (71, -0.001) 
row 71: (70, -0.001)  (71, 1.002)  (72, -0.001) 
row 72: (71, -0.001)  (72, 1.002)  (73, -0.001) 
row 73: (72, -0.001)  (73, 1.002)  (74, -0.001) 
row 74: (73, -0.001)  (74, 1.002)  (75, -0.001) 
row 75: (74, -0.001)  (75, 1.002)  (76, -0.001) 
row 76: (75, -0.001)  (76, 1.002)  (77, -0.001) 
row 77: (76, -0.001)  (77, 1.002)  (78, -0.001) 
row 78: (77, -0.001)  (78, 1.002)  (79, -0.001) 
row 79: (78, -0.001)  (79, 1.002)  (80, -0.001) 
row 80: (79, -0.001)  (80, 1.002)  (81, -0.001) 
row 81: (80, -0.001)  (81, 1.002)  (82, -0.001) 
row 82: (81, -0.001)  (82, 1.002)  (83, -0.001) 
row 83: (82, -0.001)  (83, 1.002)  (84, -0.001) 
row 84: (83, -0.001)  (84, 1.002)  (85, -0.001) 
row 85: (84, -0.001)  (85, 1.002)  (86, -0.001) 
row 86: (85, -0.001)  (86, 1.002)  (87, -0.001) 
row 87: (86, -0.001)  (87, 1.002)  (88, -0.001) 
row 88: (87, -0.001)  (88, 1.002)  (89, -0.001) 
row 89: (88, -0.001)  (89, 1.002)  (90, -0.001) 
row 90: (89, -0.001)  (90, 1.002)  (91, -0.001) 
row 91: (90, -0.001)  (91, 1.002)  (92, -0.001) 
row 92: (91, -0.001)  (92, 1.002)  (93, -0.001) 
row 93: (92, -0.001)  (93, 1.002)  (94, -0.001) 
row 94: (93, -0.001)  (94, 1.002)  (95, -0.001) 
row 95: (94, -0.001)  (95, 1.002)  (96, -0.001) 
row 96: (95, -0.001)  (96, 1.002)  (97, -0.001) 
row 97: (96, -0.001)  (97, 1.002)  (98, -0.001) 
row 98: (97, -0.001)  (98, 1.002)  (99, -0.001) 
row 99: (98, -0.001)  (99, 1.002)  (100, -0.001) 
row 100: (99, -0.001)  (100, 1.002) 
Vec Object: 1 MPI processes
  type: seq
0.
3.14108e-09
6.27905e-09
9.41083e-09
1.25333e-08
1.56434e-08
1.87381e-08
2.18143e-08
2.4869e-08
2.78991e-08
3.09017e-08
3.38738e-08
3.68125e-08
3.97148e-08
4.25779e-08
4.5399e-08
4.81754e-08
5.09041e-08
5.35827e-08
5.62083e-08
5.87785e-08
6.12907e-08
6.37424e-08
6.61312e-08
6.84547e-08
7.07107e-08
7.28969e-08
7.50111e-08
7.70513e-08
7.90155e-08
8.09017e-08
8.27081e-08
8.44328e-08
8.60742e-08
8.76307e-08
8.91007e-08
9.04827e-08
9.17755e-08
9.29776e-08
9.40881e-08
9.51057e-08
9.60294e-08
9.68583e-08
9.75917e-08
9.82287e-08
9.87688e-08
9.92115e-08
9.95562e-08
9.98027e-08
9.99507e-08
1e-07
9.99507e-08
9.98027e-08
9.95562e-08
9.92115e-08
9.87688e-08
9.82287e-08
9.75917e-08
9.68583e-08
9.60294e-08
9.51057e-08
9.40881e-08
9.29776e-08
9.17755e-08
9.04827e-08
8.91007e-08
8.76307e-08
8.60742e-08
8.44328e-08
8.27081e-08
8.09017e-08
7.90155e-08
7.70513e-08
7.50111e-08
7.28969e-08
7.07107e-08
6.84547e-08
6.61312e-08
6.37424e-08
6.12907e-08
5.87785e-08
5.62083e-08
5.35827e-08
5.09041e-08
4.81754e-08
4.5399e-08
4.25779e-08
3.97148e-08
3.68125e-08
3.38738e-08
3.09017e-08
2.78991e-08
2.4869e-08
2.18143e-08
1.87381e-08
1.56434e-08
1.25333e-08
9.41083e-09
6.27905e-09
3.14108e-09
0.
Vec Object: 1 MPI processes
  type: seq
0.
0.00318601
0.00636571
0.00953912
0.0127031
0.0158546
0.0189904
0.0221075
0.0252028
0.0282732
0.0313157
0.0343273
0.0373051
0.040246
0.0431472
0.0460058
0.048819
0.0515841
0.0542982
0.0569588
0.0595632
0.0621088
0.064593
0.0670136
0.069368
0.071654
0.0738692
0.0760116
0.0780789
0.0800692
0.0819805
0.0838109
0.0855585
0.0872218
0.0887989
0.0902884
0.0916889
0.0929988
0.094217
0.0953422
0.0963733
0.0973093
0.0981492
0.0988924
0.0995379
0.100085
0.100534
0.100883
0.101133
0.101283
0.101333
0.101283
0.101133
0.100883
0.100534
0.100085
0.0995379
0.0988924
0.0981492
0.0973093
0.0963733
0.0953422
0.094217
0.0929988
0.0916889
0.0902884
0.0887989
0.0872218
0.0855585
0.0838109
0.0819805
0.0800692
0.0780789
0.0760116
0.0738692
0.071654
0.069368
0.0670136
0.064593
0.0621088
0.0595632
0.0569588
0.0542982
0.0515841
0.048819
0.0460058
0.0431472
0.040246
0.0373051
0.0343273
0.0313157
0.0282732
0.0252028
0.0221075
0.0189904
0.0158546
0.0127031
0.00953912
0.00636571
0.00318601
0.
************************************************************************************************************************
***             WIDEN YOUR WINDOW TO 120 CHARACTERS.  Use 'enscript -r -fCourier9' to print this document            ***
************************************************************************************************************************

---------------------------------------------- PETSc Performance Summary: ----------------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


./implicit_heat.out on a  named r01n06 with 1 processor, by mae-lizj Wed Jun  8 22:10:13 2022
Using Petsc Release Version 3.16.6, Mar 30, 2022 

                         Max       Max/Min     Avg       Total
Time (sec):           1.144e+03     1.000   1.144e+03
Objects:              3.100e+01     1.000   3.100e+01
Flop:                 1.963e+11     1.000   1.963e+11  1.963e+11
Flop/sec:             1.715e+08     1.000   1.715e+08  1.715e+08
Memory:               4.440e+05     1.000   4.440e+05  4.440e+05
MPI Messages:         0.000e+00     0.000   0.000e+00  0.000e+00
MPI Message Lengths:  0.000e+00     0.000   0.000e+00  0.000e+00
MPI Reductions:       0.000e+00     0.000

Flop counting convention: 1 flop = 1 real number operation of type (multiply/divide/add/subtract)
                            e.g., VecAXPY() for real vectors of length N --> 2N flop
                            and VecAXPY() for complex vectors of length N --> 8N flop

Summary of Stages:   ----- Time ------  ----- Flop ------  --- Messages ---  -- Message Lengths --  -- Reductions --
                        Avg     %Total     Avg     %Total    Count   %Total     Avg         %Total    Count   %Total
 0:      Main Stage: 1.1443e+03 100.0%  1.9629e+11 100.0%  0.000e+00   0.0%  0.000e+00        0.0%  0.000e+00   0.0%

------------------------------------------------------------------------------------------------------------------------
See the 'Profiling' chapter of the users' manual for details on interpreting output.
Phase summary info:
   Count: number of times phase was executed
   Time and Flop: Max - maximum over all processors
                  Ratio - ratio of maximum to minimum over all processors
   Mess: number of messages sent
   AvgLen: average message length (bytes)
   Reduct: number of global reductions
   Global: entire computation
   Stage: stages of a computation. Set stages with PetscLogStagePush() and PetscLogStagePop().
      %T - percent time in this phase         %F - percent flop in this phase
      %M - percent messages in this phase     %L - percent message lengths in this phase
      %R - percent reductions in this phase
   Total Mflop/s: 10e-6 * (sum of flop over all processors)/(max time over all processors)
------------------------------------------------------------------------------------------------------------------------


      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


Event                Count      Time (sec)     Flop                              --- Global ---  --- Stage ----  Total
                   Max Ratio  Max     Ratio   Max  Ratio  Mess   AvgLen  Reduct  %T %F %M %L %R  %T %F %M %L %R Mflop/s
------------------------------------------------------------------------------------------------------------------------

--- Event Stage 0: Main Stage

VecView                3 1.0 7.4029e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecDot           180003450 1.0 5.2750e+01 1.0 3.62e+10 1.0 0.0e+00 0.0e+00 0.0e+00  5 18  0  0  0   5 18  0  0  0   686
VecNorm          120000865 1.0 3.1476e+01 1.0 2.41e+10 1.0 0.0e+00 0.0e+00 0.0e+00  3 12  0  0  0   3 12  0  0  0   766
VecScale         120000865 1.0 4.0111e+01 1.0 1.21e+10 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0   302
VecCopy          60000002 1.0 1.9086e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
VecSet           60000023 1.0 1.8472e+01 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  2  0  0  0  0   2  0  0  0  0     0
VecAXPY          240003452 1.0 8.0491e+01 1.0 4.85e+10 1.0 0.0e+00 0.0e+00 0.0e+00  7 25  0  0  0   7 25  0  0  0   602
VecMAXPY         30000001 1.0 1.3853e+01 1.0 1.82e+10 1.0 0.0e+00 0.0e+00 0.0e+00  1  9  0  0  0   1  9  0  0  0  1312
VecAssemblyBegin 30000003 1.0 4.2537e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecAssemblyEnd   30000003 1.0 4.2284e+00 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
VecPointwiseMult 120000865 1.0 4.1366e+01 1.0 1.21e+10 1.0 0.0e+00 0.0e+00 0.0e+00  4  6  0  0  0   4  6  0  0  0   293
VecNormalize     120000865 1.0 1.3170e+02 1.0 3.62e+10 1.0 0.0e+00 0.0e+00 0.0e+00 12 18  0  0  0  12 18  0  0  0   275
MatMult          90000864 1.0 7.9941e+01 1.0 4.51e+10 1.0 0.0e+00 0.0e+00 0.0e+00  7 23  0  0  0   7 23  0  0  0   564
MatAssemblyBegin       1 1.0 0.0000e+00 0.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatAssemblyEnd         1 1.0 2.8133e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
MatView                1 1.0 7.9012e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSetUp               1 1.0 2.6488e-04 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
KSPSolve         30000001 1.0 1.0345e+03 1.0 1.90e+11 1.0 0.0e+00 0.0e+00 0.0e+00 90 97  0  0  0  90 97  0  0  0   184
KSPGMRESOrthog   90000864 1.0 2.7884e+02 1.0 7.25e+10 1.0 0.0e+00 0.0e+00 0.0e+00 24 37  0  0  0  24 37  0  0  0   260
PCSetUp                1 1.0 2.1935e-05 1.0 0.00e+00 0.0 0.0e+00 0.0e+00 0.0e+00  0  0  0  0  0   0  0  0  0  0     0
PCApply          120000865 1.0 9.3377e+01 1.0 1.21e+10 1.0 0.0e+00 0.0e+00 0.0e+00  8  6  0  0  0   8  6  0  0  0   130
------------------------------------------------------------------------------------------------------------------------

Memory usage is given in bytes:

Object Type          Creations   Destructions     Memory  Descendants' Mem.
Reports information only for process 0.

--- Event Stage 0: Main Stage

              Vector    19              1         2432     0.
              Matrix     1              0            0     0.
              Viewer     4              2         1680     0.
       Krylov Solver     1              0            0     0.
      Preconditioner     1              0            0     0.
    Distributed Mesh     1              0            0     0.
   Star Forest Graph     2              0            0     0.
     Discrete System     1              0            0     0.
           Weak Form     1              0            0     0.
========================================================================================================================
Average time to get PetscTime(): 0.
#PETSc Option Table entries:
-ksp_atol 1.0e-50
-ksp_gmres_modifiedgramschmidt
-ksp_gmres_restart 30
-ksp_max_it 1500
-ksp_rtol 1.0e-10
-ksp_type gmres
-log_view
-pc_type jacobi
#End of PETSc Option Table entries
Compiled without FORTRAN kernels
Compiled with full precision matrices (default)
sizeof(short) 2 sizeof(int) 4 sizeof(long) 8 sizeof(void*) 8 sizeof(PetscScalar) 8 sizeof(PetscInt) 4
Configure options: --prefix=/work/mae-lizj/lib/petsc-3.16.6 --with-blaslapack-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl --with- debugging=no --download-hypre --download-metis --download-hdf5=/work/mae-lizj/HPC_testing/homework4/hdf5-1.12.1.tar.gz --with-mpi-dir=/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64 COPTFLAGS="-O3 -march=native -mtune=native" CXXPTFLAGS="-O3 -march=native - mtune=native" FOPTFLAGS="-O3 -march=native -mtune=native"
-----------------------------------------
Libraries compiled on 2022-05-08 14:25:15 on login03 
Machine characteristics: Linux-3.10.0-862.el7.x86_64-x86_64-with-redhat-7.5-Maipo
Using PETSc directory: /work/mae-lizj/lib/petsc-3.16.6
Using PETSc arch: 
-----------------------------------------

Using C compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiicc  -fPIC -wd1572 -Wno-unknown-pragmas -O3 -march=native -mtune=native  -std=c99 
Using Fortran compiler: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiifort  -fPIC -O3 -march=native -mtune=native     -std=c99
-----------------------------------------

Using include paths: -I/work/mae-lizj/lib/petsc-3.16.6/include -I/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/include
-----------------------------------------

Using C linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiicc
Using Fortran linker: /share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/bin/mpiifort
Using libraries: -Wl,-rpath,/work/mae-lizj/lib/petsc-3.16.6/lib -L/work/mae-lizj/lib/petsc-3.16.6/lib -lpetsc -Wl,-rpath,/work/mae-lizj/lib/petsc-3.16.6/lib -L/work/mae-lizj/lib/petsc-3.16.6/lib -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mkl/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib/release_mt -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/mpi/intel64/lib -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.4 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64_lin/gcc4.4 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/daal/lib/intel64_lin -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/daal/lib/intel64_lin -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/tbb/lib/intel64/gcc4.4 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/ipp/lib/intel64 -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/ipp/lib/intel64 -Wl,-rpath,/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin -L/share/intel/2018u4/compilers_and_libraries_2018.5.274/linux/compiler/lib/intel64_lin -Wl,-rpath,/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -L/usr/lib/gcc/x86_64-redhat-linux/4.8.5 -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib/release_mt -Wl,-rpath,/opt/intel/mpi-rt/2017.0.0/intel64/lib -lHYPRE -lmkl_intel_lp64 -lmkl_core -lmkl_sequential -lpthread -lhdf5_hl -lhdf5 -lmetis -lX11 -lstdc++ -ldl -lmpifort -lmpi -lmpigi -lrt -lpthread -lifport -lifcoremt_pic -limf -lsvml -lm -lipgo -lirc -lgcc_s -lirc_s -lquadmath -lstdc++ -ldl
-----------------------------------------



      ##########################################################
      #                                                        #
      #                       WARNING!!!                       #
      #                                                        #
      #   This code was compiled with a debugging option.      #
      #   To get timing results run ./configure                #
      #   using --with-debugging=no, the performance will      #
      #   be generally two or three times faster.              #
      #                                                        #
      ##########################################################


